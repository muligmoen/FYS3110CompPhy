\documentclass[11pt,a4paper,english]{article}
\usepackage[english]{babel} % Using babel for norwegian hyphenation
\usepackage{lmodern} % Changing the font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%\usepackage[moderate]{savetrees} % [subtle/moderate/extreme] really compact writing
\usepackage{tcolorbox}
\usepackage[parfill]{parskip} % Removes indents
\usepackage{amsmath} % Environment, symbols etc...
\usepackage{amssymb}
%\usepackage{wasysym} % Astrological symbols
\usepackage{graphicx} % For pictures etc...
\usepackage{enumitem} % Points/lists
\usepackage{physics} % Typesetting of mathematical physics examples: \bra{}, \ket{}, expval{}

% To include code(-snippets) with æøå
%\usepackage{listings}
%\lstset{
%language=python,
%showspaces=false,
%showstringspaces=false,
%frame=l,
%literate=%
%{æ}{{\ae}}1
%{ø}{{\o}}1
%{å}{{\aa}}1
%{Æ}{{\AE}}1
%{Ø}{{\O}}1
%{Å}{{\AA}}1
%}

\tolerance = 5000 % Bedre tekst
\hbadness = \tolerance
\pretolerance = 2000

\newcommand{\conj}[1]{#1^*}
\newcommand{\ve}[1]{\mathbf{#1}} % Vektorer i bold
\let\oldhat\hat
\renewcommand{\hat}[1]{\mathbf{\oldhat{#1}}}
\newcommand{\trans}[1]{#1^\top}
\newcommand{\herm}[1]{#1^\dagger}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}} % Gir fotnote-symboler
\newcommand{\Real}{\mathbb{R}}


\newcommand{\spac}{\hspace{5mm}}

\title{FYS3150/FYS4150\\Computational Physics\\Project 1}
\author{Magnus Ulimoen\\Krister Stræte Karlsen}
\date{\today}

\begin{document}
\maketitle

\section{Motivation}

Solutions of the Poissons equation are a vital component to many aspects physics.

\subsection{Electromagnetism}

The electrostatic potential $\Phi$ is generated from a charge distribution $\rho$.
This gives
\begin{equation}
\nabla^2\Phi = -4\pi \rho(\ve{r})
\end{equation}

Letting $\rho$ be spherically symmetric this leads to a symmetric $\Phi$.
This gives with a substitution $\Phi(r) = \phi(r)/r$
\begin{equation}
\frac{d^2\phi}{dr^2} = -4\pi r\rho(r)
\end{equation}
Which is solved in this project.

\section{Discretization}

Solving the equation
\begin{equation}
\frac{d^2v}{dr^2} = f(r)
\end{equation}

Letting r be discretized into N pieces,
\begin{gather}
r_0, r_1, \dots, r_i, \dots, r_{N-1}
\end{gather}
And using the three-point formula for the second derivative of f,
\begin{equation}
\frac{d^2v}{dx^2} \simeq \frac{v_{i-1} + v_{i+1} - 2v_i}{h^2}
\end{equation}

This gives the discretized formula
\begin{gather}
\frac{v_{i-1} + v_{i+1} - 2v_i}{h^2} = f(x_i)\\
v_{i-1} + v_{i+1} - 2v_i = \tilde{f}_i
\end{gather}
With boundary conditions (dropping of an element)
\begin{gather}
v_1 - 2v_0 = f_0\\
v_{N-2} - 2v_{N-1} = f_{N-1}
\end{gather}


\section{Thomas-algorithm}

\subsection{Formulation}

Given the following ODE with boundary conditions, 

\begin{equation}
-u''(x) = f(x), \spac x \in (0,1), \spac u(0) = u(1) = 0.
\end{equation}

and the dicretized form following a symmetric Taylor expansion

\begin{equation}
-\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i  \spac \mathrm{for} \spac i=1,\dots, n, \spac v_0 = v_{n} = 0
\label{eq:discretized}
\end{equation}

we are going to show it can be written as system of linear equations of the form: 

\begin{equation}
   \ve{A}\ve{v} = \tilde{\ve{b}},
   \label{eq:Avb}
\end{equation}


\subsection{Solution}
Multipling the discretized equation \eqref{eq:discretized} by $h^2$ we get:

\begin{gather*}
   -v_{i-1}+2v_{i}-v_{i+1}=h^2 f_i \hspace{0.5cm} \mathrm{for} \hspace{0.5cm} i=1,\dots, n 
\end{gather*}

Filling in for $i$ and choosing $\tilde{b_i} = h^2 f_i$ we obtain the following set of equations: 
\begin{align*}
	2v_{1}-v_{2}=\tilde{b_1} \\
	-v_{1}+2v_{2}-v_{3}=\tilde{b_2} \\
	\vdots \hspace{0.5cm} \\
	-v_{i-1}+2v_{i}-v_{i+1}=\tilde{b_i} \\
	\vdots \hspace{0.5cm}  \\ 
	-v_{n-1}+2v_{n}=\tilde{b_n} \\
\end{align*}
Now one can easily see that this system of linear equations can written on the form of \eqref{eq:Avb},
where 
\begin{gather*}
    \ve{A} = \begin{pmatrix}%{cccccc}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{pmatrix},\spac \ve{v} = \begin{pmatrix}
                           v_1\\
                           v_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           v_n\\
                      \end{pmatrix},
  \spac \tilde{\ve{b}} = \begin{pmatrix}
                           \tilde{b}_1\\
                           \tilde{b}_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           \tilde{b}_n\\
                      \end{pmatrix}.
\end{gather*}



\subsection{Algorithm for a tridiagonal system of linear equations}

We start by looking at the system of equations:
\begin{align*}
	 b_1 v_1 + c_1 v_{2} = \tilde{b_1} \hspace{0.5cm} (1*)\\
	 a_2 v_1 + b_2 v_2 + c_3 v_3 = \tilde{b_2} \hspace{0.5cm} (2*) \\
	 a_3 v_2 + b_3 v_3 + c_3 v_4 = \tilde{b_3} \hspace{0.5cm} (3*) \\
	 \vdots \hspace{0.5cm}  \\ 
	 a_n v_{n-1} + b_n v_n = \tilde{b_n} \hspace{0.5cm} (n*)\\
\end{align*}
If we solve (1*) for $v_1$ and insert it into (2*) we obtain the following "modified second equation":
\begin{align*}
	(b_1 b_2 - a_2 c_1)v_2 + b_1 c_2 v_3 = b_1 \tilde{b_2} - a_2  \tilde{b_1}  
\end{align*}
Now having successfully removed $v_1$ from the second equation we can go on and solve it for $v_2$ and insert it into the third equation obtaining:
\begin{align*}
(b_3 (b_1 b_2 - a_2 c_1)- a_3 b_1 c_2)v_3 + c_3(b_1 b_2 -a_2 c_1)v_4 = (b_1 b_2 - a_2 c_1 ) \tilde{b_3} - a_3 b_1 \tilde{b_2} + a_2 a_3 \tilde{b_1} 
\end{align*}

The two modified equations may be written as 

\begin{align*}
v_2 = \frac{b_1 \tilde{b_2} - a_2 \tilde{b_1}}{b_1 b_2 - a_2 c_1} - \frac{b_1 c_2}{b_1 b_2 - a_2 c_1} v_3 = \beta_2 + \gamma_2 v_3
\end{align*}
\begin{align*}
v_3 =& \frac{(b_1 b_2 - a_2 c_1) \tilde{b_3} - a_3 (b_1 \tilde{b_2} - a_2 \tilde{b_1})}{b_3 (b_1 b_2 - a_2 c_1)- a_3 b_1 c_2} - \frac{c_3(b_1 b_2 - a_2 c_1)}{b_3 (b_1 b_2 - a_2 c_1)- a_3 b_1 c_2} v_4 \\
=&  \frac{\tilde{b_3}-a_3 \beta_2}{a_3 \gamma_2 + b_3} + \frac{-c_3}{a_3 \gamma_2 + b_3}v_4 = \beta_3 + \gamma_3 v_4
\end{align*}

This prossess can be repeated up untill the last equation. This is the forward substitution step. From the last equation we compute $v_n$ and get all we need to compute $v_{n-1}$, then $v_{n-2}$, and so on. This is the backward substitution part of the algorithm. A shrewd reader might see that the coefficiants, $\beta$ and $\gamma$, take a recursive form
\begin{align*}
\beta_{i} = \frac{\tilde{b_i}-a_i \beta_{i-1}}{a_i \gamma_{i-1} + b_i} , \hspace{0.5cm} \gamma_{i}=\frac{-c_i}{a_i \gamma_{i-1} + b_i},
\end{align*}
and the equation for $v_{i-1}$ reads: 
\begin{equation}
v_{i-1} = \beta_{i-1} + \gamma_{i-1} v_i
\end{equation}

It follows from (1*) that $\beta_1 = \frac{\tilde{b_1}}{b_1}$ and $\gamma_1 = \frac{-c_1}{b_1}$.
From combining (n*) and (4) we get 
\begin{align*}
v_n = \frac{\tilde{b_n}-a_n \beta_{n-1}}{a_n \gamma_{n-1} + b_n} = \beta_{n}.
\end{align*}

Having all the nessecary ingredients the algorithm reads as follows.
\vspace{0.5cm}\\
\centerline{Algorithm I}
\begin{tcolorbox}
$a_i = c_i = -1, \hspace{0.5cm}  i=1,2,3,..,n$ \\
$b_i = 2, \hspace{0.5cm}  i=1,2,3,..,n $\\
$\tilde{b_i} = h^2 f_i \hspace{0.5cm}  i=1,2,3,..,n $ \\
$\beta_1 = \frac{\tilde{b_1}}{b_1}$ \\
$\gamma_1 = \frac{-c_1}{b_1}$ \\
for $i=2,3,..,n$ \\ \vspace{0.5cm} 
 \hspace{0.5cm} $ \beta_{i} = \frac{\tilde{b_{i-1}}-a_{i-1} \beta_{i-1}}{a_{i-1} \gamma_{i-1} + b_{i-1}} , \hspace{0.5cm} \gamma_{i}=\frac{-c_{i-1}}{a_{i-1} \gamma_{i-1} + b_{i-1}} $ \vspace{0.2cm}  \\
 $v_{n} = \beta_{n}$ \\
for $i=n, n-1,..,2$ \\ \vspace{0.5cm} 
 \hspace{0.5cm} $v_{i-1} = \beta_{i-1} + \gamma_{i-1} v_i$

\end{tcolorbox} 


This is often refered to as \emph{Thomas Algorithm}, an algorithm for solving tridiagonal systems of linear equations.

\subsection{FLOPS}

\section{Matrix solutions}

Can solve the equation \eqref{eq:Avb} directly by matrix solvers.

\subsection{FLOPS}
How many flops? Memory constraints, sparse matrices


\section{Implementation}

\subsection{Thomas algorithm}


\subsection{Matrix}

Using the armadillo library for the solution...

\subsection{Sparse matrix}

The matrix solution above is very ineffective in both FLOPS and memory usage.
This is improved by using sparse matrices, which saves the position and the value,
and assumes everything else is zero, which saves a lot of space in the 
memory for our purpose ($(N-3)^2$ space saved).

\section{Efficiency}

Comparing the different algorithms for different h and against each other

Compare relative error. What is rel-error compared to FLOPS?

\end{document}
