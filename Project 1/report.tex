\documentclass[11pt,a4paper,english,draft]{article}
\usepackage[english]{babel} % Using babel for norwegian hyphenation
\usepackage{lmodern} % Changing the font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%\usepackage[moderate]{savetrees} % [subtle/moderate/extreme] really compact writing
\usepackage{tcolorbox}
\usepackage[parfill]{parskip} % Removes indents
\usepackage{amsmath} % Environment, symbols etc...
\usepackage{amssymb}
%\usepackage{wasysym} % Astrological symbols
\usepackage{graphicx} % For pictures etc...
\usepackage{enumitem} % Points/lists
\usepackage{physics} % Typesetting of mathematical physics examples: \bra{}, \ket{}, expval{}

\definecolor{red}{RGB}{255,10,10}

% To include code(-snippets) with æøå
%\usepackage{listings}
%\lstset{
%language=python,
%showspaces=false,
%showstringspaces=false,
%frame=l,
%literate=%
%{æ}{{\ae}}1
%{ø}{{\o}}1
%{å}{{\aa}}1
%{Æ}{{\AE}}1
%{Ø}{{\O}}1
%{Å}{{\AA}}1
%}

\tolerance = 5000 % Bedre tekst
\hbadness = \tolerance
\pretolerance = 2000

\numberwithin{equation}{section}

\newcommand{\conj}[1]{#1^*}
\newcommand{\ve}[1]{\mathbf{#1}} % Vektorer i bold
\let\oldhat\hat
\renewcommand{\hat}[1]{\mathbf{\oldhat{#1}}}
\newcommand{\trans}[1]{#1^\top}
\newcommand{\herm}[1]{#1^\dagger}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}} % Gir fotnote-symboler
\newcommand{\Real}{\mathbb{R}}
\newcommand{\bigO}[1]{\mathcal{O}\left( #1 \right)}

\newcommand{\spac}{\hspace{5mm}}

\title{FYS3150/4150\\Computational Physics\\Project 1}
\author{Magnus Ulimoen\\Krister Stræte Karlsen}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

A great deal of differential equations in the natural sciences can be written 
as a linear second-order differential equation on the form
\begin{equation}
\frac{d^y}{dx^2} + k^2(x)y = f(x)
\label{eq:2orderDIFF}
\end{equation}


Some examples include
\begin{enumerate}[label=\bfseries \Roman*]
\item Schrödinger's equation
\item Airy function
\item Economics?
\item Poisson's equation
\item Laplace's equation (Poisson's with f=0)
\end{enumerate}

So being able to solve such equations optimally with respect to accuracy and efficiency is undoubtedly important. This report will compare different algorithms for solving them.

\textcolor{red}{KSK: More specific info on which algorithms to be investigated. }  	


\subsection{A closer look at an example from Electromagnetism}

The electrostatic potential $\Phi$ is generated from a charge distribution $\rho$.
This gives
\begin{equation}
\nabla^2\Phi = -4\pi \rho(\ve{r})
\end{equation}

Letting $\rho$ be spherically symmetric leads to a symmetric $\Phi$.
This gives with a substitution $\Phi(r) = \phi(r)/r$
\begin{equation}
\frac{d^2\phi}{dr^2} = -4\pi r\rho(r)
\end{equation}
Which is on the form of \eqref{eq:2orderDIFF}.

\section{Method}

The specific differential equation, with boundery values, being discussed in this report is: 

\begin{equation}
-u''(x) = f(x), \spac x \in (0,1), \spac u(0) = u(1) = 0.
\end{equation}

This boundery value problem can be discretized and written as a system of linear equations.

Letting the domain $x \in [0,1]$ be discretized into $n+1$ pieces,
\begin{gather}
x_0, x_1, \dots, x_i, \dots, x_{n+1}
\end{gather}
where 
\begin{gather}
h = \frac{x_{n+1} - x_0}{n+1} = \frac{x_{n+1}}{n+1}\\
x_i = x_0 + ih = ih
\end{gather}

an then sampling the solution at the mesh points so that $u(x_i) \simeq v_i$.

And using the three-point formula from the symmetric 
Taylor-expansion for the second derivative of v,
\begin{equation}
\frac{d^2v}{dx^2} \simeq \frac{v_{i-1} + v_{i+1} - 2v_i}{h^2}
\end{equation}

The discretized form of equation (2.1) can then be written as
\begin{gather}
\frac{v_{i-1} + v_{i+1} - 2v_i}{h^2} = \tilde{f}_i , \quad i=1,2, \dots, n 
\end{gather}
with $\tilde{f}_i = f(r_i) h^2$.

With boundary conditions 
\begin{gather}
v_0 = v_{N-1} = 0
\end{gather}

This can now be written as a system of linear equations on the form 

\begin{equation}
   \ve{A}\ve{v} = \tilde{\ve{b}},
   \label{eq:Avb}
\end{equation}

Multipling the discretized equation (2.6) by $h^2$ we get:

\begin{gather*}
   -v_{i-1}+2v_{i}-v_{i+1}=h^2 f_i \hspace{0.5cm} \mathrm{for} \hspace{0.5cm} i=1,\dots, n 
\end{gather*}

Filling in for $i$ and choosing $\tilde{b_i} = h^2 f_i$ we obtain the following set of equations: 
\begin{align*}
	2v_{1}-v_{2}=\tilde{b_1} \\
	-v_{1}+2v_{2}-v_{3}=\tilde{b_2} \\
	\vdots \hspace{0.5cm} \\
	-v_{i-1}+2v_{i}-v_{i+1}=\tilde{b_i} \\
	\vdots \hspace{0.5cm}  \\ 
	-v_{n-1}+2v_{n}=\tilde{b_n} \\
\end{align*}
Now one can easily see that this system of linear equations can written on the form of \eqref{eq:Avb}
\begin{gather*}
    \ve{A} = \begin{pmatrix}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{pmatrix},\spac \ve{v} = \begin{pmatrix}
                           v_1\\
                           v_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           v_n\\
                      \end{pmatrix},
  \spac \tilde{\ve{b}} = \begin{pmatrix}
                           \tilde{b}_1\\
                           \tilde{b}_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           \tilde{b}_n\\
                      \end{pmatrix}.
\end{gather*}

Having a complete set of linear equations we move on to how to solve them.

\subsection{A tridiagonal matrix algorithm}


We start by looking at the system of equations:
\begin{align}
	 b_1 v_1 + c_1 v_{2} = \tilde{b_1} \label{eq:1star}\\
	 a_2 v_1 + b_2 v_2 + c_3 v_3 = \tilde{b_2} \label{eq:2star}\\
	 a_3 v_2 + b_3 v_3 + c_3 v_4 = \tilde{b_3} \label{eq:3star}\\
\nonumber\vdots \hspace{0.5cm}  \\ 
	 a_n v_{n-1} + b_n v_n = \tilde{b_n} \label{eq:Nstar}
\end{align}
If we solve \eqref{eq:1star} for $v_1$ and insert it into \eqref{eq:2star} we obtain the following "modified second equation":
\begin{align*}
	(b_1 b_2 - a_2 c_1)v_2 + b_1 c_2 v_3 = b_1 \tilde{b_2} - a_2  \tilde{b_1}  
\end{align*}
Now having successfully removed $v_1$ from the second equation we can go on and solve it for $v_2$ and insert it into the third equation obtaining:
\begin{align*}
(b_3 (b_1 b_2 - a_2 c_1)- a_3 b_1 c_2)v_3 + c_3(b_1 b_2 -a_2 c_1)v_4 = \\
(b_1 b_2 - a_2 c_1 ) \tilde{b_3} - a_3 b_1 \tilde{b_2} + a_2 a_3 \tilde{b_1} 
\end{align*}

The two modified equations may be written as 

\begin{align*}
v_2 = \frac{b_1 \tilde{b_2} - a_2 \tilde{b_1}}{b_1 b_2 - a_2 c_1} - \frac{b_1 c_2}{b_1 b_2 - a_2 c_1} v_3 = \beta_2 + \gamma_2 v_3
\end{align*}
\begin{align*}
v_3 =& \frac{(b_1 b_2 - a_2 c_1) \tilde{b_3} - a_3 (b_1 \tilde{b_2} - a_2 \tilde{b_1})}{b_3 (b_1 b_2 - a_2 c_1)- a_3 b_1 c_2} - \frac{c_3(b_1 b_2 - a_2 c_1)}{b_3 (b_1 b_2 - a_2 c_1)- a_3 b_1 c_2} v_4 \\
=&  \frac{\tilde{b_3}-a_3 \beta_2}{a_3 \gamma_2 + b_3} + \frac{-c_3}{a_3 \gamma_2 + b_3}v_4 = \beta_3 + \gamma_3 v_4
\end{align*}

This prossess can be repeated up untill the last equation. 
This is the forward substitution step. 
From the last equation we compute $v_n$ and get all we need to compute $v_{n-1}$,
then $v_{n-2}$, and so on. This is the backward substitution part of the algorithm. A shrewd reader might see that the coefficiants, $\beta$ and $\gamma$, take a recursive form
\begin{align*}
\beta_{i} = \frac{\tilde{b_i}-a_i \beta_{i-1}}{a_i \gamma_{i-1} + b_i} , \hspace{0.5cm} \gamma_{i}=\frac{-c_i}{a_i \gamma_{i-1} + b_i},
\end{align*}
and the equation for $v_{i-1}$ reads: 
\begin{equation}
v_{i-1} = \beta_{i-1} + \gamma_{i-1} v_i
\label{eq:viminus1}
\end{equation}

It follows from \eqref{eq:1star} that $\beta_1 = \frac{\tilde{b_1}}{b_1}$ and $\gamma_1 = \frac{-c_1}{b_1}$.
From combining \eqref{eq:Nstar} and \eqref{eq:viminus1} we get 
\begin{align*}
v_n = \frac{\tilde{b_n}-a_n \beta_{n-1}}{a_n \gamma_{n-1} + b_n} = \beta_{n}.
\end{align*}

Having all the nessecary ingredients the algorithm reads as follows.
\vspace{0.5cm}\\

\centerline{Algorithm I}
\begin{tcolorbox}
$a_i = c_i = -1, \hspace{0.5cm}  i=1,2,3,..,n$\\
$b_i = 2, \hspace{0.5cm}  i=1,2,3,..,n $\\
$\tilde{b_i} = h^2 f_i \hspace{0.5cm}  i=1,2,3,..,n $\\
$\beta_1 = \frac{\tilde{b_1}}{b_1}$ \\
$\gamma_1 = \frac{-c_1}{b_1}$ \\
for $i=2,3,..,n$ \\ \vspace{0.5cm} 
 \hspace{0.5cm} $ \beta_{i} = \frac{\tilde{b_{i-1}}-a_{i-1} \beta_{i-1}}{a_{i-1} \gamma_{i-1} + b_{i-1}} , \hspace{0.5cm} \gamma_{i}=\frac{-c_{i-1}}{a_{i-1} \gamma_{i-1} + b_{i-1}} $ \vspace{0.2cm}  \\
 $v_{n} = \beta_{n}$\\
for $i=n, n-1,..,2$\\ \vspace{0.5cm} 
 \hspace{0.5cm} $v_{i-1} = \beta_{i-1} + \gamma_{i-1} v_i$

\end{tcolorbox} 


This is often refered to as \emph{Thomas Algorithm}, an algorithm for solving tridiagonal systems of linear equations. 

\textcolor{red}{KSK: More on implementation needed?}

\subsection{Standard Gaussian Elimination}

\textcolor{red}{KSK: A short description should be added here. }

\subsection{LU decomposition}

\textcolor{red}{KSK: A short description should be added here. }

\subsection{Method using sparse(?)}

\textcolor{red}{KSK: A short description should be added here. }

\section{Results}

\subsection{Speed/Efficiency}

\subsection{Accuracy/Error}

\subsection{FLOPS}

The forward substitution requires one pass forwards, and one pass backwards,
and have a complexity of $\bigO{N}$

\section{Discussion}

Can solve the equation \eqref{eq:Avb} directly by LU-decomposition.
The matrix A is then transformed into one upper triagonal matrix and
one lower diagonal matrix. By doing this on a matrix this results in an
easier way to solve several $Ax = y$ for different y.

Direct solver by arma::solve(A, y) is used to compare with the LU-method.
This is ordinary quicker, since armadillo is a high-level wrapper around
lapack etc.

\subsection{FLOPS}
The standard is $\bigO{N^3}$ complexity


\section{Implementation}

\subsection{Thomas algorithm}

Allocation of temporary arrays to hold bprime and dprime...

\subsection{Matrix}

Using the armadillo library for the solution...

\subsection{Sparse matrix}

The matrix solution above is very ineffective in both FLOPS and memory usage.
This is improved by using sparse matrices, which saves the position and the value,
and assumes everything else is zero, which saves a lot of space in the 
memory for our purpose ($(N-3)^2$ memory space saved).

\section{Concluding remarks}

Comparing the different algorithms for different h and against each other

Compare relative error. What is rel-error compared to FLOPS?

\end{document}
